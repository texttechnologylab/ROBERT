
<div align="center">
  <img src="https://github.com/TheItCrOw/R.O.B.E.R.T./assets/49918134/0415f32d-c8e0-4664-b47c-154e3382da37"/>
  <hr/>
  <h1>An open-source instruction-following large language chatting model that self-instructs itself into your specific domain.</h1>
</div
![ROBERT Inverted Color 800x600]()
![ROBERT White logo 800x600]()

[![License](https://img.shields.io/badge/Status-Under%20construction-red)]()
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

This is the repository for R.O.B.E.R.T., a chatting assistant that self-instructs itself into a specificially provided context by generating the needed datasets itself and then finetuning the LLaMa model on these datasets.
This project wouldn't be possible without:

- [Lighting-AI/lit-llama](https://github.com/Lightning-AI/lit-llama)
- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
- [openai/chatgpt](https://openai.com/blog/chatgpt)
- [MetaAI/LLaMa](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- [PEGASUS](https://github.com/google-research/pegasus)

Special thanks also to the [Text Technology Lab](https://www.texttechnologylab.org/) of the Goethe University Frankfurt from which department this project originates from. 

# ![ROBERT HAND ONLY Symbol_SMALL 800x600](https://github.com/TheItCrOw/R.O.B.E.R.T./assets/49918134/0de526b4-b39f-4159-b25c-397af3a55602) About

This repository aims to provide a streamlined process for generating a language model that has the ability to assist and chat with users in a specific domain context: <b>Your own R.O.B.E.R.T.</b> 

The datasets for the context will be generated by the methods provided in the [Self-Instruct paper](https://arxiv.org/abs/2212.10560), which were also used by the [Stanford Alpaca project](https://github.com/tatsu-lab/stanford_alpaca).

This project takes these approaches even further by: 
1. Not relying solely on chatgpt for the self-instructions (avoiding additional costs) and instead using a language model that *runs locally on your home CPU*.
2. Adding different dataset generation techniques to minimize the generation of new datasets (such as paraphrasing and chatting generation)
3. Using a finetuning method that can be easily executed on e.g. a Google VM with low GPU cost.

ðŸš© The end goal of this project includes having all steps to generate and use your own R.O.B.E.R.T. available on your home PC and therefore rendering the need for larger GPU enviroments obsolete. 

ðŸ§  Additionally, we lay our focus on a language model that is knowledgeable on only a very specific context. In contrast to other assistant-style models, which have a broad general knowledge that is often tainted with misinformation and bias, R.O.B.E.R.T. is an expert in the defined field, but knows its limitations and therefore excuses itself when the given prompt exceeds this context domain.

ðŸ’¬ Finally, we wish to provide an assistant that is capable of organic, proactive chatting and dialog, in contrast to being just an instruction-following model. 

# ![ROBERT HAND ONLY Symbol_SMALL 800x600](https://github.com/TheItCrOw/R.O.B.E.R.T./assets/49918134/0de526b4-b39f-4159-b25c-397af3a55602) Process


We have created a R.O.B.E.R.T. for a fictional scenario in a virtual reality application. In this fictional scenario, "Rob" is a virtual reality assistant that helps students of the TTL Corporation college about anything related to their day to day buisness. To do so, we wrote down bulletpoints of this fictive scenario that we want Rob to know:

```
The Va. Si. Li. Lab is a virtual reality teaching platform made by the Text Technology Lab. It simulates real life scenarios in Virtual Reality.
We are currently in the bureau of TTL Corporation.
TTL Corporation is a college. It has 100 associates and 500 students.
[...27 more]
```
Next, the datasets will be generated. Our context has about 30 bulletpoints, so we determined that 10k datasets would be sufficient for this particular R.O.B.E.R.T. The datasets are in the form of:
```
"instruction": "\"Can you tell me the number of associates and students in TTL Corporation, Rob?\"",
"input": "",
"output": "\"TTL Corporation has 100 associates and 500 students.\""
```

Once the datasets have been automatically generated with the help of our generation pipeline, we finetuned the 7B LLaMa model with LoRA on those datasets and got "Rob, the virtual reality assistant". The finetuning took 5-6h on a NVIDIA L4 with `24GB Memory`.

## Examples
### Example for instruction-following answers.
For these, no dialog was started. Rob was instructed on different occasions without previous context. 
<table align="center">
    <thead>
        <tr>
            <th align="left">Student</th>
            <th align="center">Answer Rob</th>
            <th align="right">Correctness</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="left">Where are we?</td>
            <td align="center">You are in the bureau of TTL Corporation.</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">What is the Va. Si. Li. Lab?</td>
            <td align="center">The Va. Si. Li. Lab is a virtual reality teaching platform made by the Text Technology Lab. It simulates real life scenarios in Virtual Reality.</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">List all rooms of TTL Corporation</td>
            <td align="center">Room A13, Room A14, and Room A15 exist.</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">What is a good movie you would recommend?</td>
            <td align="center">I'm sorry, I'm not sure what a good movie would be. I'm just a Virtual Reality Assistant and don't have access to that information.</td>
            <td align="right">âœ…</td>
        </tr>
    </tbody>
</table>

### Example for instruction-following in a context-based dialog.
For this example, a "student of TTL Corporation" of the fictional scenario talks with Rob in a dialog. Therefore a context and chat history was developed. 
<table align="center">
    <thead>
        <tr>
            <th align="left">Student</th>
            <th align="center">Answer Rob</th>
            <th align="right">Correctness</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="left">Hi, who are you?</td>
            <td align="center">I am Rob a Virtual Reality Assistant.</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">Can you help me?</td>
            <td align="center">Of course, I can help you. What can I do for you?</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">I need to find a room</td>
            <td align="center">Where do you want me to take you?</td>
            <td align="right">âœ…</td>
        </tr>
        <tr>
            <td align="left">Room A13. Can you guide me there?</td>
            <td align="center">Room A13 is at the end of the hallway.</td>
            <td align="right">âœ…</td>
        </tr>
         <tr>
            <td align="left">What equipment does this room offer?</td>
            <td align="center">$${\color{green}\text{This room has a projector and a whiteboard. It does not have a computer.}}$$ (recognizes the mentioned Room A13 from the chat history)</td>
            <td align="right">âœ…</td>
        </tr>
    </tbody>
</table>
